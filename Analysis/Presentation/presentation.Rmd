<<<<<<< HEAD
---
title: "Group A: Wine Reviews"
author: 'Yaxin Deng, Xiaomeng Huang, Anderson Nelson, Min Sun'
date: "April 25 2019"
output: slidy_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r library, include=FALSE, warning=FALSE}
library(data.table)
library(tidyverse)
library(tm)
library(stringr)
library(quanteda)
library(irlba)
library(ggplot2)
library(caret)
library(tidytext)
library(qdap)
library(Hmisc)
library(wordcloud)
library(wordcloud2)
library(DT)
library(png)
library(recommenderlab)
```

```{r read data+constant, echo=FALSE}
dat <- fread(input = '../Data/winemag-data-130k-v2.csv',verbose = FALSE,na.strings=c(""))

description.name="description"
points.name="points"
points.group.name="points.group"

country.name <- "country"
description.name <- "description"           
designation.name <- "designation"           
points.name <- "points"               
price.name <- "price"                 
province.name <- "province"              
region_1.name <- "region_1"              
region_2.name <- "region_2"             
taster.name <- "taster_name"           
twitter.name <- "taster_twitter_handle" 
title.name <- "title"                 
variaty.name <- "variety"              
winery.name <- "winery"               

dat[,eval(points.name)] <- as.numeric(dat[,get(points.name)])
dat[,eval(price.name)] <- as.numeric(dat[,get(price.name)])

single.var <- c(points.name, price.name, country.name)
```

```{r function}
round.numerics <- function(x, digits = 0, nearest = 1){
  if(is.numeric(x)){
    return(nearest * round(x = x/nearest, digits = digits))
  }
  else{
    return(x)
  }
}

## create formula
create.formula <- function(outcome.name, input.names, input.patterns=NA, all.data.names=NA, return.as="character"){
  variable.names.from.patterns <- c()
  if (!is.na(input.patterns[1]) & !is.na(all.data.names[1])) {
    pattern <- paste(input.patterns, collapse = "|")
    variable.names.from.patterns <- all.data.names[grep(pattern = pattern, x = all.data.names)]
  }
  all.input.names <- unique(c(input.names, variable.names.from.patterns))
  all.input.names <- all.input.names[all.input.names !=outcome.name]
  if (!is.na(all.data.names[1])) {
    all.input.names <- all.input.names[all.input.names %in% all.data.names]
  }
  input.names.delineated <- sprintf("`%s`", all.input.names)
  the.formula <- sprintf("`%s` ~ %s", outcome.name, paste(input.names.delineated, collapse = " + "))
  if (return.as == "formula") {
    return(as.formula(the.formula))
  }
  if (return.as != "formula") {
    return(the.formula)
  }
}

reduce.formula <- function(dat, the.initial.formula, max.categories = NA) {
  require(data.table)
  dat <- setDT(dat)
  
  the.sides <- strsplit(x = the.initial.formula, split = "~")[[1]]
  lhs <- trimws(x = the.sides[1], which = "both")
  lhs.original <- gsub(pattern = "`", replacement = "", x = lhs)
  if (!(lhs.original %in% names(dat))) {
    return("Error: Outcome variable is not in names(dat).")
  }
  the.pieces.untrimmed <- strsplit(x = the.sides[2], split = "+", fixed = TRUE)[[1]]
  the.pieces.untrimmed.2 <- gsub(pattern = "`", replacement = "", x = the.pieces.untrimmed, fixed = TRUE)
  the.pieces.in.names <- trimws(x = the.pieces.untrimmed.2, which = "both")

  the.pieces <- the.pieces.in.names[the.pieces.in.names %in% names(dat)]
  num.variables <- length(the.pieces)
  include.pieces <- logical(num.variables)
  
  for (i in 1:num.variables) {
    unique.values <- dat[, unique(get(the.pieces[i]))]
    num.unique.values <- length(unique.values)
    if (num.unique.values >= 2) {
      include.pieces[i] <- TRUE
    }
    if (!is.na(max.categories)) {
      if (dat[, is.character(get(the.pieces[i])) | is.factor(get(the.pieces[i]))] == TRUE) {
        if (num.unique.values > max.categories) {
          include.pieces[i] <- FALSE
        }
      }
    }
  }
  pieces.rhs <- sprintf("`%s`", the.pieces[include.pieces == TRUE])
  rhs <- paste(pieces.rhs, collapse = " + ")
  the.formula <- sprintf("%s ~ %s", lhs, rhs)
  return(the.formula)
}

# calculating relative term frequency (TF)
term.frequency <- function(row) {
  row / sum(row)
}

# calculating inverse document frequency (IDF)
inverse.doc.freq <- function(col) {
  corpus.size <- length(col)
  doc.count <- length(which(col > 0))

  log10(corpus.size / doc.count)
}

# calculating TF-IDF.
tf.idf <- function(x, idf) {
  x * idf
}
```



```{r include=FALSE}
# filter data set
subdat <- dat %>%
 select(variaty.name,country.name,points.name,price.name,province.name, winery.name,description.name)


# count length of 
subdat$descriptioncount <- str_count(string = subdat$description, pattern = '\\S+')
```

## Introduction

We used Wine Reviews  data which was scraped from *WineEntusiast* website on November 22nd, 2017 by @zackthoutt on Kaggle. There are 13 variables in total:

- **Country:** The country that the wine is from 
- **Designation:** The vineyard within the winery where the grapes that made the wine are from
- **Points:** The number of points WineEnthusiast rated the wine on a scale of 1-100 
- **Price:** The cost for a bottle of the wine
- **Province:** The province or state that the wine is from
- **Region_1:** The wine growing area in a province or state (ie Napa)
- **Region_2:** Sometimes there are more specific regions specified within a wine growing area (ie Rutherford inside the Napa Valley), but this value can sometimes be blank
- **Taster_name**
- **Taster_twitter_handle**
- **Title:** The title of the wine review, which often contains the vintage if you're interested in extracting that feature
- **Variety:** The type of grapes used to make the wine (ie Pinot Noir)
- **Winery:** The winery that made the wine

## Project Goals and Deliverables

- What are the factors that influence prices?
- Create a recommendation system that that utilizes historical data from the reviwer to recommend wines that they are likely to enjoy
- Extract insights from the reviews using NLP techniques
  - Discover the abstract “topics” that occur in the user reviews 
  - Classify the wine based on description of flavor
- Create a model that predicts price and point and evaluate on a test using RMSE and accuracy respectively? 
- Quantify the impact of wine description on price and point? 
   - Hypothesis: user description is signficant in predicting points, and not significant when predicting price?

```{r include=FALSE}
breaks <- fivenum(dat$price)
lables <- c("very cheap","cheap","normal","expensive")
price.level <- factor(cut(dat$price,breaks,lables))
dat.new <- cbind(dat,price.level)
```

## Exploratory Data Analysis

We divided the prices into quantiles and provided those quantiles with the respective labels `.

The con for this approach is that the dataset has a wide distribution and may not accurately reflect the best cluster for, however each quantile has an equal amount of numeric values. 

To improve this analysis we are considering a cluster analysis to evaluate the categories. 

```{r}
ggplot(dat.new[!is.na(price.level)], aes(price.level, points, fill = price.level)) + 
  geom_boxplot(notch = T,notchwidth = 0.5) + 
  ggtitle("Points vs. Price Level") +
  labs(x="Price Level",y="Points") +
  geom_violin()+
  scale_fill_brewer(palette = "Accent") +
  labs(fill = "price.level")

table(breaks)
```


## Visualization and Insight: 
Length of word distribution for 3 select countries
```{r warning=FALSE}
# Description distribution
subdat %>%
  filter(country %in% c('Italy','Portugal','US')) %>%
  ggplot(aes(x = country, y= points)) + geom_boxplot(fill = 'royalblue4') +  
  theme_classic() +
  ggtitle('Points Boxplot of 3 countries') + 
  xlab('Country') + 
  ylab('Points') +
  theme(plot.title = element_text(hjust = 0.5)) + ylim(80, 100)
```


## Text Analytics


```{r}
# Sentiment Analysis
subdat %>%
unnest_tokens(output = word, input = description) %>%
inner_join(get_sentiments('nrc')) %>%
group_by(sentiment) %>%
count() %>%
ggplot(aes(x=reorder(sentiment,-n),y=n, fill=sentiment)) + geom_col() + 
 theme_classic() + guides(fill=F) + 
 ggtitle('Sentiment Distribution') + ylab('Number of Obversation') + 
 theme(plot.title = element_text(hjust = 0.5)) + xlab('Sentiment: Emotion') + 
  geom_text(aes(label = n),vjust = -.5, nudge_y = 1.4)
```

## Sentimenent Analysis by country

```{r warning=FALSE}
# Sentiment Analysis by country 

write.dat.csv <- subdat %>%
  unnest_tokens(output = word, input = description) %>%
  inner_join(get_sentiments('bing')) %>%
  dplyr::group_by(country,sentiment) %>%
  summarise(n = n())%>%
  mutate(proportion = n/sum(n))


ggplot(data = write.dat.csv, aes(x= country, y=proportion,fill=sentiment)) + geom_col() + 
  theme_classic() + guides(fill=F) + coord_flip() + 
  ggtitle('Sentiment Distribution') + ylab('Number of Oversation') + 
  theme(plot.title = element_text(hjust = 0.5))


```


## Text analytics 

- Step 1: How many words in every review?

```{r, warning=FALSE}
textdat <- dat[,.(description=as.character(get(description.name)),
                  points=as.numeric(get(points.name)))]
textdat <- textdat[!(is.na(get(points.name))),]
```

```{r text_length, warning=FALSE}
textdat=textdat[,textlen:=as.numeric(nchar(description))]
# quantile(textdat$points,probs=seq(0,1,0.25))
cuts.points <- c(80,86,88,91,100)
textdat[, eval(points.group.name):=cut2(x=get(points.name), cuts=cuts.points)]

ggplot(textdat,aes(x=textlen,fill=points.group))+
  theme_classic()+
  geom_histogram(binwidth = 5,stat="count")
```

## Text Analysis

- Step 2: Data Pipeline
  
  > Tokenization
  > Remove "stopwords"
  > Stem the words
  > Create a document-feature matrix

```{r pipeline ,echo = TRUE}
# cut reviews into single words
text.tokens=tokens(textdat$description,what="word",remove_numbers=TRUE, remove_punct=TRUE,remove_symbols = TRUE, remove_hyphens = TRUE) 
# remove unwanted words
text.tokens=tokens_select(text.tokens,stopwords(),selection = "remove" ) 
# word stem
text.tokens=tokens_wordstem(text.tokens,language = "english") 
# Create dfm
text.tokens.dfm <- dfm(text.tokens, tolower = FALSE)
```

## Text Analysis

- Step 3: WordCloud

```{r wordcloud}
wordcloudData <- data.table(word=colnames(text.tokens.dfm),freq=colSums(text.tokens.dfm))
setorderv(wordcloudData,cols="freq",order=-1)
wordcloud2(data=wordcloudData,size=1,minSize=0.1)
```

## Text Analysis

- Step 4: Term Frequency-Inverse Document Frequency (TF-IDF)

TF-IDF is a powerful technique for enhancing the information/signal contained within our document-frequency matrix.

  > Normalize all documents in the corpus to be length independent
  > Penalize terms occur frequency across corpus

Based on wordcloud plot, we selected top 100 frequetent words. Then we delected some meaningless words, such as "wine","well","show" and etc. In the end, we chose 58 words as our new variables. 

```{r TF-IDF}
var.chosen<-c('fruit','acid','palat','aroma','cherri','tannin','ripe','dri','spice','rich','fresh','berri','oak','plum','textur','sweet','appl','full','blend','balanc','bodi','blackberri','light','soft','age','structur','white','crisp','fruiti','citrus','dark','miner','herb','Cabernet','raspberri','vanilla','bright','pepper','firm','green','lemon','juici','peach','concentr','pear','chocol','currant','Pinot','smooth','spici','wood','lime','intens','tart','tannic','tight','herbal','orang')

# Transform to a matrix and inspect.
text.tokens.matrix <- as.matrix(text.tokens.dfm[,var.chosen])

# First step, normalize all documents via TF.
train.tokens.df.norm <- apply(text.tokens.matrix, 1, term.frequency)

# Second step, calculate the IDF vector that we will use - both
# for training data and for test data!
text.tokens.idf <- apply(text.tokens.matrix, 2, inverse.doc.freq)

# Lastly, calculate TF-IDF for our training corpus.
text.tokens.tfidf <-  apply(train.tokens.df.norm, 2, tf.idf, idf = text.tokens.idf)
datatable(data=round(text.tokens.tfidf[1:10, 1:6],2), rownames=TRUE)
```


## Text Analysis

- Step 5: Prediction



```{r train_test}
#dim(text.tokens.matrix)
text.tokens.df <- cbind(points = textdat$points, data.frame(text.tokens.matrix))

set.seed(423)
indexes=createDataPartition(text.tokens.df$points,times=1,p=0.7,list=F) # 70% as train data 
train=text.tokens.df[indexes,]
test=text.tokens.df[-indexes,]
```

```{r model}
formula.points <- create.formula(outcome.name="points",input.names=var.chosen)
the.formula <- reduce.formula(dat = train, the.initial.formula = formula.points)
model <- lm(formula = the.formula, data=train)
summary(model)
```

## Text Analysis

- Step 5: Prediction(cont'd)

```{r}
test$points.est <- predict(model,newdata=test)
test$points.diff <- test$points.est-test$points
perc <- round(100*sum(abs(test$points.diff)<=2,na.rm=TRUE)/nrow(test),2)
```

If we allow a samll difference between actual points and estimated points (eg:abs(diff)=2), the accuracy rate is `r perc`%.

```{r check_performance}
ggplot(data = test,aes(x= points.diff,y=..density..)) + 
  geom_histogram(na.rm=TRUE,fill="royalblue4",bins=30) +
  geom_vline(xintercept =-2, col="red",lty=2,lwd=1.5) +
  geom_vline(xintercept = 2, col="red",lty=2,lwd=1.5) +
  theme_classic() +
  ggtitle('Histogram of Residuals') + 
  xlab('Residuals') + 
  ylab('Probability') + 
  theme(plot.title = element_text(hjust = 0.5)) 
```

## Building a recommendation system 

Since, users tend to vary in their pattern of evaluation (positivity bias, negativity bias, consistent ratings), it
is a good practice to standardize data (center, Z-score)

```{r echo = TRUE}
dat <- dat %>%
 select(taster_name, variety,points)

dat <- dat[complete.cases(dat)]

# convert into a s4 class 
data_matrix <- as(dat, 'realRatingMatrix')
image(data_matrix[1:19,1:50])

# z score normalization 
data_matrix_znorm <-normalize(x = data_matrix, method='Z-score')
image(data_matrix_znorm[1:19,1:50])

# center normalization 
data_matrix_cnorm <-normalize(x = data_matrix, method='center')
image(data_matrix_cnorm[1:19,1:50])
```

## Building a recommendation system: Generating Predictions, normal data set 
Generating top 2 movie recommendations uding the popular method on the unmodified dataset. 

```{r}

recommenderRegistry$get_entry("POPULAR", type ="realRatingMatrix")
# recommendation for normal data 
recom <- Recommender(data_matrix,method='POPULAR',parameter = list(normalize=NULL))
pred <- predict(recom,data_matrix, n = 2)

recommendation.popular1 <- as.data.frame(mapply(c, getList(pred), 
                     getRatings(pred), 
                     as.list(rowMeans(data_matrix, na.rm = TRUE))))

datatable(recommendation.popular1[1:2,])

```

## Building a recommendation system: Generating Predictions, normalized data set
Generating top 2 movie recommendations uding the popular method on the normalize dataset. 

```{r}
# recommendation for normalized data by z score
recom2 <- Recommender(data_matrix,method='POPULAR',parameter = list(normalize=NULL))
pred2 <- predict(recom,data_matrix_znorm, n = 2)
recommendation.popular2 <- as.data.frame(mapply(c, getList(pred2), 
                     getRatings(pred2), 
                     as.list(rowMeans(data_matrix_znorm, na.rm = TRUE))))

datatable(recommendation.popular2[1:2,])
```

## Building a recommendation system: Generating Predictions, z score data set data set
Generating top 2 movie recommendations uding the popular method on the dataset scaled using the Z score. 
```{r}
# recommendation for normalized data by center
recom3 <- Recommender(data_matrix,method='POPULAR',parameter = list(normalize=NULL))
pred3 <- predict(recom,data_matrix_znorm, n = 2)
recommendation.popular3 <- as.data.frame(mapply(c, getList(pred3), 
                     getRatings(pred3), 
                     as.list(rowMeans(data_matrix_znorm, na.rm = TRUE))))

datatable(recommendation.popular3[1:2,])
```

## Conclusion: 

Al three methods produces slightly different recommendations, next step is to evalate the results on a test set. 


## Thank You 
=======
---
title: "Group A: Wine Reviews"
author: 'Yaxin Deng, Xiaomeng Huang, Anderson Nelson, Min Sun'
date: "April 25 2019"
output: slidy_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r library, include=FALSE, warning=FALSE}
library(data.table)
library(tidyverse)
library(tm)
library(stringr)
library(quanteda)
library(irlba)
library(ggplot2)
library(caret)
library(tidytext)
library(qdap)
library(Hmisc)
library(wordcloud)
library(wordcloud2)
library(DT)
library(png)
library(recommenderlab)
```

```{r read data+constant, echo=FALSE}
dat <- fread(input = '../Data/winemag-data-130k-v2.csv',verbose = FALSE,na.strings=c(""))

description.name="description"
points.name="points"
points.group.name="points.group"

country.name <- "country"
description.name <- "description"           
designation.name <- "designation"           
points.name <- "points"               
price.name <- "price"                 
province.name <- "province"              
region_1.name <- "region_1"              
region_2.name <- "region_2"             
taster.name <- "taster_name"           
twitter.name <- "taster_twitter_handle" 
title.name <- "title"                 
variaty.name <- "variety"              
winery.name <- "winery"               

dat[,eval(points.name)] <- as.numeric(dat[,get(points.name)])
dat[,eval(price.name)] <- as.numeric(dat[,get(price.name)])

single.var <- c(points.name, price.name, country.name)
```

```{r function}
round.numerics <- function(x, digits = 0, nearest = 1){
  if(is.numeric(x)){
    return(nearest * round(x = x/nearest, digits = digits))
  }
  else{
    return(x)
  }
}

## create formula
create.formula <- function(outcome.name, input.names, input.patterns=NA, all.data.names=NA, return.as="character"){
  variable.names.from.patterns <- c()
  if (!is.na(input.patterns[1]) & !is.na(all.data.names[1])) {
    pattern <- paste(input.patterns, collapse = "|")
    variable.names.from.patterns <- all.data.names[grep(pattern = pattern, x = all.data.names)]
  }
  all.input.names <- unique(c(input.names, variable.names.from.patterns))
  all.input.names <- all.input.names[all.input.names !=outcome.name]
  if (!is.na(all.data.names[1])) {
    all.input.names <- all.input.names[all.input.names %in% all.data.names]
  }
  input.names.delineated <- sprintf("`%s`", all.input.names)
  the.formula <- sprintf("`%s` ~ %s", outcome.name, paste(input.names.delineated, collapse = " + "))
  if (return.as == "formula") {
    return(as.formula(the.formula))
  }
  if (return.as != "formula") {
    return(the.formula)
  }
}

reduce.formula <- function(dat, the.initial.formula, max.categories = NA) {
  require(data.table)
  dat <- setDT(dat)
  
  the.sides <- strsplit(x = the.initial.formula, split = "~")[[1]]
  lhs <- trimws(x = the.sides[1], which = "both")
  lhs.original <- gsub(pattern = "`", replacement = "", x = lhs)
  if (!(lhs.original %in% names(dat))) {
    return("Error: Outcome variable is not in names(dat).")
  }
  the.pieces.untrimmed <- strsplit(x = the.sides[2], split = "+", fixed = TRUE)[[1]]
  the.pieces.untrimmed.2 <- gsub(pattern = "`", replacement = "", x = the.pieces.untrimmed, fixed = TRUE)
  the.pieces.in.names <- trimws(x = the.pieces.untrimmed.2, which = "both")

  the.pieces <- the.pieces.in.names[the.pieces.in.names %in% names(dat)]
  num.variables <- length(the.pieces)
  include.pieces <- logical(num.variables)
  
  for (i in 1:num.variables) {
    unique.values <- dat[, unique(get(the.pieces[i]))]
    num.unique.values <- length(unique.values)
    if (num.unique.values >= 2) {
      include.pieces[i] <- TRUE
    }
    if (!is.na(max.categories)) {
      if (dat[, is.character(get(the.pieces[i])) | is.factor(get(the.pieces[i]))] == TRUE) {
        if (num.unique.values > max.categories) {
          include.pieces[i] <- FALSE
        }
      }
    }
  }
  pieces.rhs <- sprintf("`%s`", the.pieces[include.pieces == TRUE])
  rhs <- paste(pieces.rhs, collapse = " + ")
  the.formula <- sprintf("%s ~ %s", lhs, rhs)
  return(the.formula)
}

# calculating relative term frequency (TF)
term.frequency <- function(row) {
  row / sum(row)
}

# calculating inverse document frequency (IDF)
inverse.doc.freq <- function(col) {
  corpus.size <- length(col)
  doc.count <- length(which(col > 0))

  log10(corpus.size / doc.count)
}

# calculating TF-IDF.
tf.idf <- function(x, idf) {
  x * idf
}
```



```{r include=FALSE}
# filter data set
subdat <- dat %>%
 select(variaty.name,country.name,points.name,price.name,province.name, winery.name,description.name)


# count length of 
subdat$descriptioncount <- str_count(string = subdat$description, pattern = '\\S+')
```

## Introduction

We used Wine Reviews  data which was scraped from *WineEntusiast* website on November 22nd, 2017 by @zackthoutt on Kaggle. There are 13 variables in total:

- **Country:** The country that the wine is from 
- **Designation:** The vineyard within the winery where the grapes that made the wine are from
- **Points:** The number of points WineEnthusiast rated the wine on a scale of 1-100 
- **Price:** The cost for a bottle of the wine
- **Province:** The province or state that the wine is from
- **Region_1:** The wine growing area in a province or state (ie Napa)
- **Region_2:** Sometimes there are more specific regions specified within a wine growing area (ie Rutherford inside the Napa Valley), but this value can sometimes be blank
- **Taster_name**
- **Taster_twitter_handle**
- **Title:** The title of the wine review, which often contains the vintage if you're interested in extracting that feature
- **Variety:** The type of grapes used to make the wine (ie Pinot Noir)
- **Winery:** The winery that made the wine

## Project Goals and Deliverables

- What are the factors that influence prices?
- Create a recommendation system that that utilizes historical data from the reviwer to recommend wines that they are likely to enjoy
- Extract insights from the reviews using NLP techniques
  - Discover the abstract “topics” that occur in the user reviews 
  - Classify the wine based on description of flavor
- Create a model that predicts price and point and evaluate on a test using RMSE and accuracy respectively? 
- Quantify the impact of wine description on price and point? 
   - Hypothesis: user description is signficant in predicting points, and not significant when predicting price?

```{r include=FALSE}
breaks <- fivenum(dat$price)
lables <- c("very cheap","cheap","normal","expensive")
price.level <- factor(cut(dat$price,breaks,lables))
dat.new <- cbind(dat,price.level)
```

## Exploratory Data Analysis

We mainly explored varibles: country, winery, variety, designation, price, points through data visualization. See flexdashoboard for details.


## Visualization and Insight: 
Length of word distribution for 3 select countries
```{r warning=FALSE}
# Description distribution
subdat %>%
  filter(country %in% c('Italy','Portugal','US')) %>%
  ggplot(aes(x = country, y= points)) + geom_boxplot(fill = 'royalblue4') +  
  theme_classic() +
  ggtitle('Points Boxplot of 3 countries') + 
  xlab('Country') + 
  ylab('Points') +
  theme(plot.title = element_text(hjust = 0.5)) + ylim(80, 100)
```


## Text Analytics


```{r}
# Sentiment Analysis
subdat %>%
unnest_tokens(output = word, input = description) %>%
inner_join(get_sentiments('nrc')) %>%
group_by(sentiment) %>%
count() %>%
ggplot(aes(x=reorder(sentiment,-n),y=n, fill=sentiment)) + geom_col() + 
 theme_classic() + guides(fill=F) + 
 ggtitle('Sentiment Distribution') + ylab('Number of Obversation') + 
 theme(plot.title = element_text(hjust = 0.5)) + xlab('Sentiment: Emotion') + 
  geom_text(aes(label = n),vjust = -.5, nudge_y = 1.4)
```

## Sentimenent Analysis by country

```{r warning=FALSE}
# Sentiment Analysis by country 

write.dat.csv <- subdat %>%
  unnest_tokens(output = word, input = description) %>%
  inner_join(get_sentiments('bing')) %>%
  dplyr::group_by(country,sentiment) %>%
  summarise(n = n())%>%
  mutate(proportion = n/sum(n))


ggplot(data = write.dat.csv, aes(x= country, y=proportion,fill=sentiment)) + geom_col() + 
  theme_classic() + guides(fill=F) + coord_flip() + 
  ggtitle('Sentiment Distribution') + ylab('Number of Oversation') + 
  theme(plot.title = element_text(hjust = 0.5))


```


## Text analytics 

- Step 1: How many words in every review?

```{r, warning=FALSE}
textdat <- dat[,.(description=as.character(get(description.name)),
                  points=as.numeric(get(points.name)))]
textdat <- textdat[!(is.na(get(points.name))),]
```

```{r text_length, warning=FALSE}
textdat=textdat[,textlen:=as.numeric(nchar(description))]
# quantile(textdat$points,probs=seq(0,1,0.25))
cuts.points <- c(80,86,88,91,100)
textdat[, eval(points.group.name):=cut2(x=get(points.name), cuts=cuts.points)]

ggplot(textdat,aes(x=textlen,fill=points.group))+
  theme_classic()+
  geom_histogram(binwidth = 5,stat="count")
```

## Text Analysis

- Step 2: Data Pipeline
  
  > Tokenization
  > Remove "stopwords"
  > Stem the words
  > Create a document-feature matrix

```{r pipeline ,echo = TRUE}
# cut reviews into single words
text.tokens=tokens(textdat$description,what="word",remove_numbers=TRUE, remove_punct=TRUE,remove_symbols = TRUE, remove_hyphens = TRUE) 
# remove unwanted words
text.tokens=tokens_select(text.tokens,stopwords(),selection = "remove" ) 
# word stem
text.tokens=tokens_wordstem(text.tokens,language = "english") 
# Create dfm
text.tokens.dfm <- dfm(text.tokens, tolower = FALSE)
```

## Text Analysis

- Step 3: WordCloud

```{r wordcloud}
wordcloudData <- data.table(word=colnames(text.tokens.dfm),freq=colSums(text.tokens.dfm))
setorderv(wordcloudData,cols="freq",order=-1)
wordcloud2(data=wordcloudData,size=1,minSize=0.1)
```

## Text Analysis

- Step 4: Term Frequency-Inverse Document Frequency (TF-IDF)

TF-IDF is a powerful technique for enhancing the information/signal contained within our document-frequency matrix.

  > Normalize all documents in the corpus to be length independent
  > Penalize terms occur frequency across corpus

Based on wordcloud plot, we selected top 100 frequetent words. Then we delected some meaningless words, such as "wine","well","show" and etc. In the end, we chose 58 words as our new variables. 

```{r TF-IDF}
var.chosen<-c('fruit','acid','palat','aroma','cherri','tannin','ripe','dri','spice','rich','fresh','berri','oak','plum','textur','sweet','appl','full','blend','balanc','bodi','blackberri','light','soft','age','structur','white','crisp','fruiti','citrus','dark','miner','herb','Cabernet','raspberri','vanilla','bright','pepper','firm','green','lemon','juici','peach','concentr','pear','chocol','currant','Pinot','smooth','spici','wood','lime','intens','tart','tannic','tight','herbal','orang')

# Transform to a matrix and inspect.
text.tokens.matrix <- as.matrix(text.tokens.dfm[,var.chosen])

# First step, normalize all documents via TF.
train.tokens.df.norm <- apply(text.tokens.matrix, 1, term.frequency)

# Second step, calculate the IDF vector that we will use - both
# for training data and for test data!
text.tokens.idf <- apply(text.tokens.matrix, 2, inverse.doc.freq)

# Lastly, calculate TF-IDF for our training corpus.
text.tokens.tfidf <-  apply(train.tokens.df.norm, 2, tf.idf, idf = text.tokens.idf)
datatable(data=round(text.tokens.tfidf[1:10, 1:6],2), rownames=TRUE)
```


## Text Analysis

- Step 5: Prediction



```{r train_test}
#dim(text.tokens.matrix)
text.tokens.df <- cbind(points = textdat$points, data.frame(text.tokens.matrix))

set.seed(423)
indexes=createDataPartition(text.tokens.df$points,times=1,p=0.7,list=F) # 70% as train data 
train=text.tokens.df[indexes,]
test=text.tokens.df[-indexes,]
```

```{r model}
formula.points <- create.formula(outcome.name="points",input.names=var.chosen)
the.formula <- reduce.formula(dat = train, the.initial.formula = formula.points)
model <- lm(formula = the.formula, data=train)
summary(model)
```

## Text Analysis

- Step 5: Prediction(cont'd)

```{r}
test$points.est <- predict(model,newdata=test)
test$points.diff <- test$points.est-test$points
perc <- round(100*sum(abs(test$points.diff)<=2,na.rm=TRUE)/nrow(test),2)
```

If we allow a samll difference between actual points and estimated points (eg:abs(diff)=2), the accuracy rate is `r perc`%.

```{r check_performance}
ggplot(data = test,aes(x= points.diff,y=..density..)) + 
  geom_histogram(na.rm=TRUE,fill="royalblue4",bins=30) +
  geom_vline(xintercept =-2, col="red",lty=2,lwd=1.5) +
  geom_vline(xintercept = 2, col="red",lty=2,lwd=1.5) +
  theme_classic() +
  ggtitle('Histogram of Residuals') + 
  xlab('Residuals') + 
  ylab('Probability') + 
  theme(plot.title = element_text(hjust = 0.5)) 
```

## Building a recommendation system 

Since, users tend to vary in their pattern of evaluation (positivity bias, negativity bias, consistent ratings), it
is a good practice to standardize data (center, Z-score)

```{r echo = TRUE}
dat <- dat %>%
 select(taster_name, variety,points)

dat <- dat[complete.cases(dat)]

# convert into a s4 class 
data_matrix <- as(dat, 'realRatingMatrix')
image(data_matrix[1:19,1:50])

# z score normalization 
data_matrix_znorm <-normalize(x = data_matrix, method='Z-score')
image(data_matrix_znorm[1:19,1:50])

# center normalization 
data_matrix_cnorm <-normalize(x = data_matrix, method='center')
image(data_matrix_cnorm[1:19,1:50])
```

## Building a recommendation system: Generating Predictions, normal data set 
Generating top 2 movie recommendations uding the popular method on the unmodified dataset. 

```{r}

recommenderRegistry$get_entry("POPULAR", type ="realRatingMatrix")
# recommendation for normal data 
recom <- Recommender(data_matrix,method='POPULAR',parameter = list(normalize=NULL))
pred <- predict(recom,data_matrix, n = 2)

recommendation.popular1 <- as.data.frame(mapply(c, getList(pred), 
                     getRatings(pred), 
                     as.list(rowMeans(data_matrix, na.rm = TRUE))))

datatable(recommendation.popular1[1:2,])

```

## Building a recommendation system: Generating Predictions, normalized data set
Generating top 2 movie recommendations uding the popular method on the normalize dataset. 

```{r}
# recommendation for normalized data by z score
recom2 <- Recommender(data_matrix,method='POPULAR',parameter = list(normalize=NULL))
pred2 <- predict(recom,data_matrix_znorm, n = 2)
recommendation.popular2 <- as.data.frame(mapply(c, getList(pred2), 
                     getRatings(pred2), 
                     as.list(rowMeans(data_matrix_znorm, na.rm = TRUE))))

datatable(recommendation.popular2[1:2,])
```

## Building a recommendation system: Generating Predictions, z score data set data set
Generating top 2 movie recommendations uding the popular method on the dataset scaled using the Z score. 
```{r}
# recommendation for normalized data by center
recom3 <- Recommender(data_matrix,method='POPULAR',parameter = list(normalize=NULL))
pred3 <- predict(recom,data_matrix_znorm, n = 2)
recommendation.popular3 <- as.data.frame(mapply(c, getList(pred3), 
                     getRatings(pred3), 
                     as.list(rowMeans(data_matrix_znorm, na.rm = TRUE))))

datatable(recommendation.popular3[1:2,])
```

## Conclusion: 

Al three methods produces slightly different recommendations, next step is to evalate the results on a test set. 


## Thank You 
>>>>>>> 02040bd5da84ed4e8cdddc79f0ac452d35b55952
