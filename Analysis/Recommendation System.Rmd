---
title: "Recommendation System"
output:
  prettydoc::html_pretty:
    theme: leonids
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r library, include=FALSE}
library(data.table)
library(tidyverse)
library(recommenderlab)
library(DT)
library(prettydoc)
```


```{r read data, echo=FALSE}
dat <- fread(input = '../Data/wine.clean.csv',verbose = FALSE,na.strings=c(""))
```

```{r constants}
points.name <- "points"
taster.name <- "taster_name"
variety.name <- "variety"

dat <- dat[, mean(get(points.name)), by = .(get(taster.name), get(variety.name))]
setnames(dat, old = c('get','get.1','V1'), new = c('Taster Name','Variety','Point'))
```


```{r real rating class}
# convert into a s4 class 
data_matrix <- as(dat, 'realRatingMatrix')
image(data_matrix[1:19,1:50])
```

```{r train and test split}

set.seed(256)
split <- sample(x = nrow(data_matrix),size = 0.8*nrow(data_matrix))
train <- data_matrix[split,]
test <- data_matrix[-split,]


mean(getRatings(data_matrix), na.rm = TRUE)


es <- evaluationScheme(data_matrix, method = 'split', train = 0.8, given = 5)
```

### **Calculate a similarity matrix**

The similarity matrix allows you to understand how similar users are to each other. We are going to show the top 7. 

```{r similarity matrix}

# similarity matrix using the euclidean distance 
round(similarity(normalize(train[1:7]),method = 'euclidean'),3)

# similarity matrix using the cosine distance
round(similarity(normalize(train[1:7]),method = 'cosine'),3)

# similarity matrix using the pearson distance

similarity.matrix <- round(similarity(normalize(train[1:7]),method = 'pearson'),3)
```


### **User Based Collaborative Filter** 

Here, we try to search for lookalike tasters and offer wines based on what tasters with simiar taste profile has. This algorithm is very effective but takes a lot of time and resources. This type of filtering requires computing every customer pair information which takes time. So, for big base platforms, this algorithm is hard to put in place.


```{r UBCF}
recommenderRegistry$get_entry('UBCF', type = 'realRatingMatrix')

recom.ubcf <- Recommender(train,method='UBCF',parameter = list(method = 'Cosine'))
pred.ubcf <- predict(recom.ubcf,train,n = 1)

datatable(data.frame(mapply(c, 'Wine Recommendation' = getList(pred.ubcf), 
                     'Wine Rating' = getRatings(pred.ubcf),
                     'Avg Rating' = as.list(rowMeans(train)))))

recom.ubcf.2 <- Recommender( data = getData(es, 'train'),
                             method = 'UBCF')

pred.ubcf.2 <- predict(recom.ubcf.2,newdata = getData(es, 'known'), type = 'ratings')

calcPredictionAccuracy(pred.ubcf.2, data = getData(es, 'unknown'))

```

### **Popular Methods** 

The Popular method is one of the oldest approaches to recommendations involves recommmending the most popular item. Although less widely used today, we still see this in the form of Top 50 Music Hits, Billboard Top Hits, and New York Times Bestsellers. The underlying assumption is that if most people like a wine, a book or a song, you will like it too.

```{r Popular Method}
recommenderRegistry$get_entry("POPULAR", type ="realRatingMatrix")

# recommendation for normal data 
recom.popular <- Recommender(train,method='POPULAR')
pred.popular <- predict(recom.popular,data_matrix, n = 1)

# top recommendation 
datatable(
data.frame(mapply(c, 'Wine Recommendation' = getList(pred.popular), 
                  'Wine Rating' = getRatings(pred.popular),
                     'Avg Rating' = as.list(rowMeans(train)))))

recom.popular.2 <- Recommender( data = getData(es, 'train'),
                             method = 'POPULAR')

pred.popular.2 <- predict(recom.popular.2,newdata = getData(es, 'known'), type = 'ratings')

calcPredictionAccuracy(pred.popular.2, data = getData(es, 'unknown'))

```
 
### **Item Collaborative filtering** 

It is very similar to the previous algorithm, but instead of finding a tasters look alike, we try finding wine look alike. Once we have wine look alike matrix, we can easily recommend alike wine to a customer who has reviewed any wine the store. This algorithm requires far fewer resources than user-user collaborative filtering. Hence, for new tasters or customer, the algorithm takes far lesser time than user collaborate as we don’t need all similarity scores between customers. Amazon uses this approach in its recommendation engine to show related products which boost sales.


```{r}

recommenderRegistry$get_entry('IBCF', type = 'realRatingMatrix')

recom.ibcf <- Recommender(train,method='IBCF')
pred.ibcf <- predict(recom.ubcf,train, n = 1, type = 'topNList')

datatable(
data.frame(mapply(c, 'Receommendatoin' = getList(pred.ibcf), 
                     'Wine Rating' = getRatings(pred.ibcf),
                     'Avg Rating' = as.list(rowMeans(train))))
)

recom.ibcf.2 <- Recommender( data = getData(es, 'train'),
                             method = 'IBCF')

pred.ibcf.2 <- predict(recom.ibcf.2,newdata = getData(es, 'known'), type = 'ratings')

calcPredictionAccuracy(pred.ibcf.2, data = getData(es, 'unknown'))
```


The model with the lowest RMSE is Popular based method. Below we examine the problems with recommendation systems

### Challenges

**Data Sparsity**: When new items are added to system, they need to be rated by substantial number of users before they could be recommended to users who have similar tastes with the ones rated them.

**Cold Start Problem**: The “cold start" problem happens in recommendation systems due to the lack of information, on users or items. The Cold-Start problem is a wellknown issue in recommendation systems: there is relatively little information about each user, which results in an inability to draw inferences to recommend items to users. 

Lastly, the accuracy improves as the number of items per user. 














