---
title: "Text Analysis"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r library, include=FALSE}
library(data.table)
library(DT)
library(tidyverse)
library(tidytext)
library(qdap)
library(ggthemes)
library(wordcloud)
library(RColorBrewer)
library(tm)
library(stringr)
library(quanteda)
library(irlba)
library(ggplot2)
library(caret)
```


```{r read data, echo=FALSE}
dat <- fread(input = '../Data/wine.clean.csv',verbose = FALSE,na.strings=c(""))
dat <- dat[,-1]

```

```{r constants}
country.name <- "country"
description.name <- "description"           
designation.name <- "designation"           
points.name <- "points"               
price.name <- "price"                 
province.name <- "province"              
region_1.name <- "region_1"              
region_2.name <- "region_2"             
taster.name <- "taster_name"           
twitter.name <- "taster_twitter_handle" 
title.name <- "title"                 
variaty.name <- "variety"              
winery.name <- "winery"               

dat[,eval(points.name)] <- as.numeric(dat[,get(points.name)])
dat[,eval(price.name)] <- as.numeric(dat[,get(price.name)])

single.var <- c(points.name, price.name, country.name)
```



```{r}
# filter data set
subdat <- dat %>%
 select(variaty.name,country.name,points.name,province.name, winery.name,description.name)

head(subdat)
# count length of 
subdat$descriptioncount <- str_count(string = subdat$description, pattern = '\\S+')
```

```{r}
# summary of description length
summary(subdat$descriptioncount)

# Description distribution
ggplot(data = subdat, aes(x = descriptioncount)) + geom_histogram(bins=30, fill = 'royalblue4') + 
 theme_classic() +
 ggtitle('Description word length Distribution') + 
 xlab('Number of words used in Description') + 
 ylab('Number of Observations') + 
 theme(plot.title = element_text(hjust = 0.5)) + scale_fill_brewer(palette = "Blues")
 ylim(0,25000) 
```

```{r}

# top words 
subdat[, qdap::freq_terms(text.var = subdat$description, stopwords = Top200Words)]

# Sentiment Analysis
subdat %>%
unnest_tokens(output = word, input = description) %>%
inner_join(get_sentiments('bing')) %>%
group_by(sentiment) %>%
count()%>%
ggplot(aes(x=sentiment,y=n, fill=sentiment)) + geom_col() + 
 theme_classic() + guides(fill=F) + 
 ggtitle('Sentiment Distribution') + ylab('Number of Oversation') + 
 ylim(0,400000) + theme(plot.title = element_text(hjust = 0.5)) + geom_text(aes(label = n))


# Sentiment Analysis ny country 
subdat %>%
unnest_tokens(output = word, input = description) %>%
inner_join(get_sentiments('bing')) %>%
group_by(country,sentiment) %>%
summarize(n = n())%>%
mutate(proportion = n/sum(n)) %>%
ggplot(aes(x= country, y=proportion,fill=sentiment)) + geom_col() + 
 theme_classic() + guides(fill=F) + coord_flip() + 
 ggtitle('Sentiment Distribution') + ylab('Number of Oversation') + 
 theme(plot.title = element_text(hjust = 0.5)) 



```

```{r}
# Sentiment Analysis
subdat %>%
unnest_tokens(output = word, input = description) %>%
inner_join(get_sentiments('nrc')) %>%
group_by(sentiment) %>%
count() %>%
ggplot(aes(x=reorder(sentiment,-n),y=n, fill=sentiment)) + geom_col() + 
 theme_classic() + guides(fill=F) + 
 ggtitle('Sentiment Distribution') + ylab('Number of Obversation') + 
 theme(plot.title = element_text(hjust = 0.5)) + xlab('Sentiment: Emotion')


# Sentiment Analysis ny country 
subdat %>%
unnest_tokens(output = word, input = description) %>%
inner_join(get_sentiments('nrc')) %>%
group_by(country,sentiment) %>%
summarize(n = n()) %>%
mutate(proportion = n/sum(n)) %>%
ggplot(aes(x= country, y=proportion,fill=sentiment, col = sentiment)) + geom_col() + 
 theme_classic() + guides(fill=F) + coord_flip() + 
 ggtitle('Sentiment Distribution') + ylab('Number of Oversation') + 
 theme(plot.title = element_text(hjust = 0.5))




subdat %>%
unnest_tokens(output = word, input = description) %>%
inner_join(get_sentiments('nrc')) %>%
 group_by(sentiment,points) %>%
 count()%>%
 group_by(sentiment, points) %>%
 summarize(n = mean(n)) %>%
 ungroup() %>%
 ggplot(aes(x=points, y=n, fill=points)) +
  geom_col() + facet_wrap(~sentiment) +
  guides(fill=F) + coord_flip() +theme_classic() + ylab('Number of Observation') + 
  xlab('Points') + ggtitle('Emotion') + 
  theme(plot.title = element_text(hjust = 0.5))


```



```{r}

# World cloud data 
wordcloudData <- 
 subdat%>%
 unnest_tokens(output = word, input = description) %>%
 anti_join(stop_words)%>%
 group_by(word)%>%
 summarize(freq = n())%>%
 arrange(desc(freq))%>%
 ungroup()%>%
 data.frame()

wordcloud(words = wordcloudData$word,wordcloudData$freq,scale=c(2,0.5),
          max.words = 100, colors = brewer.pal(10,'Spectral'))

```



#---------------------------------------------NLP 


```{r}
textdat=dat[,.(description=as.character(get(description.name)),points)]
length(which(!complete.cases(textdat))) # check if there are incomplete cases
```

```{r}
textdat=textdat[,textlen:=nchar(description)]
textdat=textdat[,description:=tolower(description)]
fix.contractions=function(dat){
  dat=gsub("isn't", "is not",dat)
  dat=gsub("'s","",dat)
  dat=gsub("'","",dat)
  dat=gsub("wine", "",dat)
  dat=removePunctuation(dat)
  return(dat)
}

#incomplete.cases=which(!complete.cases(train.tokens.tfidf))

textdat=textdat[,lapply(.SD, FUN="fix.contractions")]
# ggplot(textdat,aes(x=textlen,fill=points))+geom_histogram(binwidth = 5,stat="count")



# set.seed(423)
# indexes=createDataPartition(textdat$points,times=1,p=0.7,list=F) # 70% as train data 
# text.train=textdat[indexes,]
# text.test=textdat[-indexes,]
text.train=textdat[1:100,] #due to memory issue, we select 100 data to test method for now

#tokenization

text.train.tokens=tokens(text.train$description,what="word") # cut reviews into single words
head(stopwords())
```

```{r}
text.train.tokens=tokens_select(text.train.tokens,stopwords(),selection = "remove" ) # remove unwanted words
text.train.tokens=tokens_wordstem(text.train.tokens,language = "english") #word-stem

head(text.train.tokens,n=3)
```

```{r}
#uni-gram and bi-gram
text.train.tokens=tokens_ngrams(text.train.tokens,n=1:2)

#bag-of-words model
train.tokens.dfm=dfm(text.train.tokens,tolower=F)
head(train.tokens.dfm)
train.tokens.matrix=as.matrix(train.tokens.dfm)

#ti-idf
term.freq=function(row){
  row/sum(row)
}

inverse.doc.frq=function(col){
  corpus.size=length(col)
  doc.count=length(which(col>0))
  
  log10(corpus.size/doc.count)
}

tf.idf=function(tf,idf){
  tf*idf
}



train.tokens.df=apply(train.tokens.matrix,1,term.freq)
train.tokens.idf=apply(train.tokens.matrix,2,inverse.doc.frq)
train.tokens.tfidf=apply(train.tokens.df,2,tf.idf,idf=train.tokens.idf)

#fix incomplete cases
incomplete.cases=which(!complete.cases(train.tokens.tfidf))
train.tokens.tfidf[incomplete.cases,]=rep(0.0,ncol(train.tokens.tfidf))
sum(which(!complete.cases(train.tokens.tfidf)))



#check incomplete cases
incomplete.cases=which(!complete.cases(train.tokens.idf))


#Clean data frame 
train.tokens.tfidf.df=cbind(points=text.train$points,data.frame(train.tokens.tfidf))
names(train.tokens.tfidf.df)=make.names(names(train.tokens.tfidf.df))


```










