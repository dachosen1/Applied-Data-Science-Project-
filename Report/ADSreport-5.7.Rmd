---
title: "Applied Data Science Finace Project: Analysis on Wine Reviews"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r library, include=FALSE}
library(data.table)
library(tidyverse)
library(tm)
library(stringr)
library(quanteda)
library(irlba)
library(ggplot2)
library(caret)
library(tidytext)
library(Hmisc)
library(wordcloud)
library(wordcloud2)
library(DT)
library(png)
library(recommenderlab)
library(corrplot)
library(foreign)
library(MASS)
library(reshape2)
library(Hmisc)
```

```{r read data, echo=FALSE,warning=FALSE}
dat <- fread(input = '../Data/winemag-data-130k-v2.csv',verbose = FALSE,na.strings=c(""))
dat=dat[,-1]
```


```{r functions}
percentage.table <- function(x, digits = 1){
  tab <- table(x)
  percentage.tab <- 100*tab/(sum(tab))
  rounded.tab <- round(x = percentage.tab, digits = digits)
  return(rounded.tab)
}
round.numerics <- function(x, digits){
  if(is.numeric(x)){
    x <- round(x = x, digits = digits)
  }
  return(x)
}
miss.value <- function(x){
  miss.rate <- round(100*mean(is.na(x)),2)
  return(miss.rate)
}
get.unique = function(x){
  ux=length(unique(x))
  return(ux)
}
## create formula
create.formula <- function(outcome.name, input.names, input.patterns=NA, all.data.names=NA, return.as="character"){
  variable.names.from.patterns <- c()
  if (!is.na(input.patterns[1]) & !is.na(all.data.names[1])) {
    pattern <- paste(input.patterns, collapse = "|")
    variable.names.from.patterns <- all.data.names[grep(pattern = pattern, x = all.data.names)]
  }
  all.input.names <- unique(c(input.names, variable.names.from.patterns))
  all.input.names <- all.input.names[all.input.names !=outcome.name]
  if (!is.na(all.data.names[1])) {
    all.input.names <- all.input.names[all.input.names %in% all.data.names]
  }
  input.names.delineated <- sprintf("`%s`", all.input.names)
  the.formula <- sprintf("`%s` ~ %s", outcome.name, paste(input.names.delineated, collapse = " + "))
  if (return.as == "formula") {
    return(as.formula(the.formula))
  }
  if (return.as != "formula") {
    return(the.formula)
  }
}

reduce.formula <- function(dat, the.initial.formula, max.categories = NA) {
  require(data.table)
  dat <- setDT(dat)
  
  the.sides <- strsplit(x = the.initial.formula, split = "~")[[1]]
  lhs <- trimws(x = the.sides[1], which = "both")
  lhs.original <- gsub(pattern = "`", replacement = "", x = lhs)
  if (!(lhs.original %in% names(dat))) {
    return("Error: Outcome variable is not in names(dat).")
  }
  the.pieces.untrimmed <- strsplit(x = the.sides[2], split = "+", fixed = TRUE)[[1]]
  the.pieces.untrimmed.2 <- gsub(pattern = "`", replacement = "", x = the.pieces.untrimmed, fixed = TRUE)
  the.pieces.in.names <- trimws(x = the.pieces.untrimmed.2, which = "both")

  the.pieces <- the.pieces.in.names[the.pieces.in.names %in% names(dat)]
  num.variables <- length(the.pieces)
  include.pieces <- logical(num.variables)
  
  for (i in 1:num.variables) {
    unique.values <- dat[, unique(get(the.pieces[i]))]
    num.unique.values <- length(unique.values)
    if (num.unique.values >= 2) {
      include.pieces[i] <- TRUE
    }
    if (!is.na(max.categories)) {
      if (dat[, is.character(get(the.pieces[i])) | is.factor(get(the.pieces[i]))] == TRUE) {
        if (num.unique.values > max.categories) {
          include.pieces[i] <- FALSE
        }
      }
    }
  }
  pieces.rhs <- sprintf("`%s`", the.pieces[include.pieces == TRUE])
  rhs <- paste(pieces.rhs, collapse = " + ")
  the.formula <- sprintf("%s ~ %s", lhs, rhs)
  return(the.formula)
}

# calculating relative term frequency (TF)
term.frequency <- function(row) {
  row / sum(row)
}

# calculating inverse document frequency (IDF)
inverse.doc.freq <- function(col) {
  corpus.size <- length(col)
  doc.count <- length(which(col > 0))

  log10(corpus.size / doc.count)
}

# calculating TF-IDF.
tf.idf <- function(x, idf) {
  x * idf
}
text.prep=function(dat,var.name){
  # tokenization
  dat.token=tokens(dat[,get(var.name)],what="word", remove_punct=T, remove_symbols=T, remove_hyphens = T)
  # lower case the tokens
  dat.token=tokens_tolower(dat.token)
  # stopword removal
  dat.token=tokens_select(dat.token,stopwords(),selection = "remove")
  # stemming
  dat.token=tokens_wordstem(dat.token,language = "english")
  return(dat.token)
}

linear.regression.summary <- function(lm.mod, digits = 3, alpha = 0.05) {
lm.coefs <- as.data.table(summary(lm.mod)$coefficients,
keep.rownames = TRUE)
setnames(x = lm.coefs, old = "rn", new = "Variable")
z <- qnorm(p = 1 - alpha/2, mean = 0, sd = 1)
lm.coefs[, Coef.Lower.95 := Estimate - z * `Std. Error`]
lm.coefs[, Coef.Upper.95 := Estimate + z * `Std. Error`]
return(lm.coefs[])
}
```


```{r constants, echo=F}
description.name="description"
points.name="points"
points.group.name="points.group"

country.name <- "country"
description.name <- "description"
designation.name <- "designation"
points.name <- "points"
price.name <- "price"
province.name <- "province"
region_1.name <- "region_1"
region_2.name <- "region_2"
taster.name <- "taster_name"
twitter.name <- "taster_twitter_handle" 
title.name <- "title"
variety.name <- "variety"
winery.name <- "winery"

dat[,eval(points.name)] <- as.numeric(dat[,get(points.name)])
dat[,eval(price.name)] <- as.numeric(dat[,get(price.name)])

single.var <- c(points.name, price.name, country.name)

price.category.range <- c(4,10,15,20,30,50,100,200)
price.category.name <- c('Value','Value Premium','Premium','Super Premium','Ultra Premium',
                         'Luxury','Super Luxury','Icon')

# Calculating unique values in the dataset
unique.designation <- length(unique(dat$designation))
unique.province <- length(unique(dat$province))
unique.variety <- length(unique(dat$variety))

designation.min <- 50
variety.min <- 200
province.min <- 500

```

# Introduction

<p> One of the most important research questions in the wine related area is the ranking, rating and judging of wine[1]. In the past, the research on these problems manily utilized samll dataset[2]. However, with data science techniques, we are able to investigate the problem using large amount of data. </p>

<p> Majority of the studies focus on analyzing physicochemical laboratory wine data, while little research has been conducted on the content of wine reviews. Mining useful information from wine reviews and tasters preferences can provide new insights from a consumer perspective and business perspective. Therefore, it is meaningful to untilize these data and generate better wine recommendations.</p>


# Source of Data
<body>
We used <b> <a href = "https://www.kaggle.com/zynicide/wine-reviews" > Wine Reviews</a></b> data which was scraped from <i>WineEntusiast</i> website on November 22nd, 2017 by @zackthoutt on Kaggle. The orginial data also had an index variable which we ignored here since it has no impact on our project. There are 13 variables in total:

<ul>
<li><b>Country</b>: The country that the wine is from; </li>

<li><b>Description</b>: wine reviews; </li>

<li><b>Designation</b>: The vineyard within the winery where the grapes that made the wine are from;</li>

<li><b>Points</b>: The number of points WineEnthusiast rated the wine on a scale of 1-100 (though they say they only post reviews for wines that score >=80);</li>

<li><b>Price</b>: The cost for a bottle of the wine; </li>

<li><b>Province</b>: The province or state that the wine is from;</li>

<li><b>Region_1</b>: The wine growing area in a province or state (ie Napa) ;</li>

<li><b>Region_2</b>: Sometimes there are more specific regions specified within a wine growing area (ie Rutherford inside the Napa Valley), but this value can sometimes be blank ;</li>

<li><b>Taster_name</b>: Name of the taster(reviewer);</li>

<li><b>Taster_twitter_handle</b>;</li>

<li><b>Title</b>: The title of the wine review, which often contains the vintage if you're interested in extracting that feature ;</li>

<li><b>Variety</b>: The type of grapes used to make the wine (ie Pinot Noir) ;</li>

<li><b>Winery</b>: The winery that made the wine.</li>

</ul>
</body>

```{r,include=F}
datatable(dat[1:5,])
```

# Examination of the Data

<p> We first checked the missing vlaues in the data and visualize the percentage of that for each varible. **Country, province, region_1 and region_2** are varibales giving geographic information about the wine, since the majority of **region_2** and 16.4% of **region_1** are missing, we choose to focus on **country** unless more detailed geographic information are needed. We also assign the value of the highest level region to the lowest level for the missing components. </p>

We then examined the unique values for each varible. **Designation, winery, title** have many unique values, hence we decided to treat them as random effects but not predictors when modeling.<br>

**Price** of the wine ranges from 4 to 3300 with some extreme values. We then visualized **price** removed all the variable that were NA. 

Variable **points** ranges from 80 to 100. Its distribution was close to a bell shape. <br>

**Description**, **taster_name** and **taster_twitter_handle** provides information about the taster and their reviews on the wine. We examined these variables later based on the needs for text anlysis and recommendation systems building. We also assumed tha the the missing values for taster name were submitted anomymously. Those value have been altered to reflect anomymous. 

**Designation** We reseached wine designation to further understand Wine Designation and according to Napa Valley Vinters: Vineyard Designations Optional. Many wineries name the vineyard where the grapes were grown because the winery believes the property produces an unusually high-quality grape. The winery or an independent grower may own the vineyard. When using a vineyard designation on a wine label, federal regulations require that 95% of the grapes be grown in the named vineyard.

```{r percentage missing}
tab  <- dat[,lapply(X=.SD,FUN="miss.value"),.SD=names(dat)[-1]]
tab2 <- data.table(var=names(dat[,-1]),miss=as.numeric(tab))

# create a plot of all the missing rate 
barplot(height=tab2$miss, names.arg=tab2$var,space=0.01, las=1, 
        main="Percentage of Missing Value",ylab="Percentage", ylim=c(0,80),
        col=rainbow(13),border="white")

text(x = -0.5 + 1.02*1:tab2[, .N], y = -15, labels = tab2$var,srt = 45)
space_val=0

text(x=-0.4+1:length(tab)*(1+space_val),y=tab,labels=sprintf("%.1f%%",tab),pos=3)

#number of unique values for each variable
unitab=dat[,lapply(.SD,FUN="get.unique")]
datatable(unitab)

#range of price
summary(dat$price)
hist(dat[,price],xlab="price", col= "#85C1E9", border="white", main="Histogram of price")

#points plot
hist(dat$points,xlab="points", col= "#85C1E9", border="white", main="Histogram of points")
```

# Data Cleaning
```{r cleaning}

# evaluate designation as no wine designation 
dat[is.na(get(designation.name)), eval('designation') := 'No Wine Designation']

# Evalaluate missing province 
dat[is.na(get(region_1.name)), eval('region_1') := get(province.name)]
dat[is.na(get(region_2.name)), eval('region_2') := get(region_1.name)]

# taster name changed to anonymous 
dat[is.na(get(taster.name)), eval('taster_name') := 'Anonymous']

# remove NA price
dat <- dat[complete.cases(dat)]
paste0( nrow(dat) - nrow(dat.final), ' rows were removed or ', round((nrow(dat) / nrow(dat.final) - 1) * 100,2),'%' , 'of the data')
```


# Investigation:
##1. Sentiment Analysis 
###Overview of *Description*

Suppose we are exploring sales status of different wines in our website. What do we care about most? The answer might be the *Final Point* the testers give out. But some reality need to be considered. 
First of all, the dataset we have only includes data with points above 80. Comparing to our scoring scale, which is 0-100, scores from 80 to 100 have a strong indication that the testers are relatively like this kind of wine. Under this circumstance, how can we specify the difference among them? 
Second, when customers evaluate a product, their evaluation objects vary. Some people focus on the taste while others prefer the looks and smell. As what we get from Final Point is just the final evaluation, we cannot tell which aspect this tester pay attention to. However, when recommending product to users, it could be more precise if you know about his or her preference on which aspect he or she focuses on. 
From the reason above, *Final Point* is not enough for us to make an accurate conclusion on customers exact evaluation on this type of wine. Thus, we should take a look at *Description* ,which gives us more details.
In order to analyze this text variable, we first take a look at the words' length. We split every sentence into words and draw a distribution of words' length. We find that it is close to a normal distribution with mean is roughly 45 words per description, which is a good start.

```{r}
# filter data set
subdat <- dat %>%
 select(variety.name,country.name,points.name,province.name, winery.name,description.name)

head(subdat)
# count length of 
subdat$descriptioncount <- str_count(string = subdat$description, pattern = '\\S+')
```

```{r distributioin of words length}
ggplot(data = subdat, aes(x = descriptioncount)) + geom_histogram(bins=30, fill = 'royalblue4') + 
 theme_classic() +
 ggtitle('Description word length Distribution') + 
 xlab('Number of words used in Description') + 
 ylab('Number of Observations') + 
 theme(plot.title = element_text(hjust = 0.5)) + scale_fill_brewer(palette = "Blues")
 ylim(0,25000) 
```

To give a accurate evaluation, we start from estimating sentiment behind descriptioin, which is sentiment analysis. There's a default classification criteria of "negative" and "positive", which helps us to roughly divide description into two categories. From the result,we get more positive descriptions than negative one which kind of makes sense because we just use data with points from 80 to 100. There, the proportion of positive description is 79% and that of negative description is roughly 21%.

The result kind of shows discrepancy. As we mention above, we shift data with points above 80, which is a relatively higher score with scale from 0 to 100. We didn't expect negative description that much. In order to have a more realistic understanding of the result, we create a new scale which is from 80 to 100 and we assume points above 90 is "positive" points while points below 90 is "negative" points.Thus, we make a prediction the distribution of new-scaled points is consistent with the description of negative and positive. However, from what we get we conclude that the prediction is not correct, which indicates that although the final point is positive, we still have negative desription. Thus, it's necessary to analyze description with more specific classification which is text analysis.

```{r sentiment analysis}
subdat %>%
unnest_tokens(output = word, input = description) %>%
inner_join(get_sentiments('bing')) %>%
group_by(sentiment) %>%
count() %>%
ggplot(aes(x=sentiment,y=n, fill=sentiment)) + geom_col() + 
 theme_classic() + guides(fill=F) + 
  scale_fill_brewer(palette = "Accent")+
  ggtitle('Sentiment Distribution') + ylab('Number of Oversation') + 
  ylim(0,400000) + theme(plot.title = element_text(hjust = 0.5)) 
```


##Sentiment analysis of different countries

There are two main factors which influence the quality of a type of wine, one is natural cause and one is man-made cause. The country of origin is the important component in natural cause. Thus, we want to specify if wines from different countries will have different descriptions from testers. With the sentiment perspective, it will be shown as the difference in the distribution of negative and positive sentiment in description.
Then, we draw a graph of sentiment distribution to see the difference among wines from different counties. As some countries’ wine do not have sufficient recorded review, we just show countries with large amount of data. 

From the result, we can tell that there exists difference among countries. Meanwhile, wines that from most of the famous places like, France, South Africa have the larger proportion of positive description which is consistent with our intuition.


```{r }
data1 <- subdat[,length(get(description.name)),by=country.name]
setorderv(data1,cols = "V1",-1)
order1 <- data1$country[1:15]

subdat[get(country.name)%in%order1,] %>%
unnest_tokens(output = word, input = description) %>%
inner_join(get_sentiments('bing')) %>%
group_by(country,sentiment) %>%
summarise(n = n())%>%
mutate(proportion = n/sum(n)) %>%
ggplot(aes(x= country , y=proportion,fill=sentiment)) + geom_col() + 
 theme_classic() + guides(fill=F) + coord_flip() + 
  scale_fill_brewer(palette = "Accent")+
 ggtitle('Sentiment Distribution') + ylab('Number of Obversation') + 
 theme(plot.title = element_text(hjust = 0.5)) 
```

##Other interesting finding

Dividing sentiment in "negative" and "positive" is somewhat sketchy and we have other way to make a more specific classfication. We extract more emotions from descriptions such as "disgust","joy" etc. To our surprise, there exists words containing sentiment like "anger","sad" which seems a little bit strange in a wine description. This interesting finding gives us more hint on our further text analysis.

```{r}
subdat %>%
unnest_tokens(output = word, input = description) %>%
inner_join(get_sentiments('nrc')) %>%
group_by(sentiment) %>%
count() %>%
ggplot(aes(x=reorder(sentiment,-n),y=n, fill=sentiment)) + geom_col() + 
 theme_grey() + guides(fill=F) + 
 ggtitle('Sentiment Distribution') + ylab('Number of Obversation') + 
 theme(plot.title = element_text(hjust = 0.5)) + xlab('Sentiment: Emotion') + 
  geom_text(aes(label = n),vjust = -.5, nudge_y = 1.4)

subdat %>%
unnest_tokens(output = word, input = description) %>%
inner_join(get_sentiments('nrc')) %>%
group_by(country,sentiment) %>%
summarise(n = n()) %>%
mutate(proportion = n/sum(n)) %>%
ggplot(aes(x= country, y=proportion,fill=sentiment, col = sentiment)) + geom_col() + 
 theme_classic() + guides(fill=F) + coord_flip() + 
 ggtitle('Sentiment Distribution') + ylab('Number of Oversation') + 
 theme(plot.title = element_text(hjust = 0.5))


subdat %>%
unnest_tokens(output = word, input = description) %>%
inner_join(get_sentiments('nrc')) %>%
 group_by(sentiment,points) %>%
 count()%>%
 group_by(sentiment, points) %>%
 summarise(n = mean(n)) %>%
 ungroup() %>%
 ggplot(aes(x=points, y=n, fill=points)) +
  geom_col() + facet_wrap(~sentiment) +
  guides(fill=F) + coord_flip() +theme_classic() + ylab('Number of Observation') + 
  xlab('Points') + ggtitle('Emotion') + 
  theme(plot.title = element_text(hjust = 0.5))

```

#2. Text Analysis

The sentiment analysis shows there exits relationship between reviews and points, which is consistent with our common sense. In this part, we continued analyzing wines' review.

##1.1 The length of reviews

The plot below shows that the wines with longer reviews tend to have higher points. So texters usually have more opinions to share when he/she thinks the wine is great. It is also worth to notice that the variance of text length tend to be bigger if the wine has higher points. It may imply that even the good wines usually have longer reviews, the length is also influenced by many other factors, such as ttesters' habits. But if the wine is not so good, testers always have less to write down.

```{r text_length, warning=FALSE}
textdat <- dat[,.(description=as.character(get(description.name)),
                  points=as.numeric(get(points.name)))]
textdat <- textdat[!(is.na(get(points.name))),]

textdat=textdat[,textlen:=as.numeric(nchar(description))]
# quantile(textdat$points,probs=seq(0,1,0.25))
cuts.points <- c(80,86,88,91,100)
textdat[, eval(points.group.name):=cut2(x=get(points.name), cuts=cuts.points)]

ggplot(textdat,aes(x=textlen,fill=points.group))+
  theme_classic()+
  geom_histogram(binwidth = 5,stat="count")
```

##2. Data Pipeline

Before analysis on text data, we did some preparation. 
  
- **Tokenization**: Cut the reviews into single words to calculate the frequency of their appearance.

- **Remove "stopwords"**: Remove "stopwords" such as "the", "a", "are", which are meaningless but necessary in sentence. 

- **Stem the words**: Stem the words in order to reduce the number of total different words appeare in the reviews. For example, this step change "fruit", "fruity", "fruits" all into "fruiti". Even though "fruiti" is not actually a word, it is still understandable.


```{r}
text.tokens=text.prep(textdat,description.name)
```

- **Create a DFM**: Create a document-feature matrix for later analysis

```{r pipeline ,echo = TRUE}
text.tokens.dfm <- dfm(text.tokens, tolower = FALSE)
```

- **WordCloud**: Use WorldCloud to see the results. "fruit", "acid", "tanni", "cherry", "dry" tend to appear more frequently than others. We should set those words as new variables.

```{r wordcloud}
wordcloudData <- data.table(word=colnames(text.tokens.dfm),freq=colSums(text.tokens.dfm))
setorderv(wordcloudData,cols="freq",order=-1)
wordcloud2(data=wordcloudData,size=1,minSize=0.1)
```

##2.3 Term Frequency-Inverse Document Frequency (TF-IDF)

TF-IDF is a powerful technique for enhancing the information contained within our document-frequency matrix. The tf-idf weight measures how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus (data-set). To complete this process, we need two steps listed below:

- Normalize all documents in the corpus to be length independent (TF)

- Penalize terms occur frequency across corpus and inverse document frequency (IDF) 

```{r TF-IDF error=recover}
var.chosen<-c('fruit','acid','palat','aroma','cherri','tannin','ripe','dri','spice','rich','fresh',
              'berri','oak','plum','textur','sweet','appl','full','blend','balanc','bodi','blackberri',
              'light','soft','age','structur','white','crisp','fruiti','citrus','dark','miner','herb',
              'Cabernet','raspberri','vanilla','bright','pepper','firm','green','lemon','juici','peach',
              'concentr','pear','chocol','currant','Pinot','smooth','spici','wood','lime','intens','tart',
              'tannic','tight','herbal','orang')

# Transform to a matrix and inspect.
text.tokens.matrix <- as.matrix(text.tokens.dfm[,var.chosen])

# First step, normalize all documents via TF.
train.tokens.df.norm <- apply(text.tokens.matrix, 1, term.frequency)

# Second step, calculate the IDF vector that we will use - both
# for training data and for test data!
text.tokens.idf <- apply(text.tokens.matrix, 2, inverse.doc.freq)

# Lastly, calculate TF-IDF for our training corpus.
text.tokens.tfidf <-  apply(train.tokens.df.norm, 2, tf.idf, idf = text.tokens.idf)

round(text.tokens.tfidf[1:10, 1:6],2)
```

For example, in the text 1, fruit has a lower score than acid. We believe that "fruit" has less information because most of reviews include "fruit". On the contrary, acidity is more useful in this case, since only part of the wines are acid, which makes it more unique and characteristic, and more suitable as a variable.

##2.4 Prediction

Based on wordcloud plot and TF-IDF, we selected top 100 frequetent words. Then we delected some meaningless words for this project, such as "wine","well","show" and etc. In the end, we chose 58 words as our new variables. 

```{r train_test}
#dim(text.tokens.matrix)
text.tokens.df <- cbind(points = textdat$points, data.frame(text.tokens.matrix))

set.seed(423)
indexes=createDataPartition(text.tokens.df$points,times=1,p=0.7,list=F) # 70% as train data 
train=text.tokens.df[indexes,]
test=text.tokens.df[-indexes,]
```

```{r model}
formula.points <- create.formula(outcome.name="points",input.names=var.chosen)
the.formula <- reduce.formula(dat = train, the.initial.formula = formula.points)
model <- lm(formula = the.formula, data=train)
summary(model)
```

F statistic is 483.7 and p-value is smaller than $2.2*10^{-16}$. Thus, the model is significant. Also, most variables are significant in this model. But adjusted R-squared is 0.2353, which is not really high. Considering that testers may not write down all aspect of a wine and the standard of valuating a wine is rather compelecated, we still think this is a nice try.

```{r predict}
test$points.est <- predict(model,newdata=test)
test$points.diff <- test$points.est-test$points
perc <- round(100*sum(abs(test$points.diff)<=2,na.rm=TRUE)/nrow(test),2)
```

Since points is continuous, we allow a samll difference between actual points and estimated points (eg:abs(diff)=2), the accuracy rate is `r perc`%.

```{r check_performance}
ggplot(data = test,aes(x= points.diff,y=..density..)) + 
  geom_histogram(na.rm=TRUE,fill="royalblue4",bins=30) +
  geom_vline(xintercept =-2, col="red",lty=2,lwd=1.5) +
  geom_vline(xintercept = 2, col="red",lty=2,lwd=1.5) +
  theme_classic() +
  ggtitle('Histogram of Residuals') + 
  xlab('Residuals') + 
  ylab('Probability') + 
  theme(plot.title = element_text(hjust = 0.5)) 
```


Limitations for text analysis:<br>
First, if we're using each unique word as a predictor to fit the model, the number of predictors will explode really fast. Even though we did data cleaning such as removing unwanted words and symbols, it's not ideal to go over every word.  <br>

Second, we built a linear regression model. Among the words with high frequency, we manully selected some words as the predictors based on the domain knowledge. For example, *fruit*,*cherri*,*plum*,*oak*,*chocol*, are the some common words used for describing the flavors of the wine. Therefore, the result is interpretable since each predictor is a meaningful word realted to the wine. However, since we're focusing on high frequency words,it's hard get a thorough exploration of descriptions. Also, as this a manually process, any change in the dataset will impact the significance of these predictors.<br>

Lastly, when building the model, all the words are coming from previous dataset which means we draw conclusions based on that. When there's a new word coming up in the descriptions, we won't be able to handle that. This is no longer the situation where a new observation brings new values for existing predictors, but the case when a new observation brings new predictors. <br>

# Areas of Future Investigation
To tackle the limitaions for text analysis in this project, we tried another approach. We first followed the same procedure for text cleaning. After getting the word-document matrix, we used SVD (singluar vector decomposition) to reduce the number of predictors. However, to use SVD, we need to fully trust the math since each column is no longer a word that we can intepret. There were signals missing in the dimension reduction process and we didn't know what those are. Then we built a decison tree model to predict points. However, our model ended up predicting the same point for all the wine, hence we ended up going back to the original method for now. Even so, this is something interesting for us to do future investigation. A plausible step is to investigate on feature engineering. <br>

#3. Recommendation System

```{r real rating class}
# convert into a s4 class 
data_matrix <- as(dat, 'realRatingMatrix')
image(data_matrix[1:19,1:50])
```

```{r train and test split}
set.seed(256)
split <- sample(x = nrow(data_matrix),size = 0.8*nrow(data_matrix))
train <- data_matrix[split,]
test <- data_matrix[-split,]

mean(getRatings(data_matrix), na.rm = TRUE)

es <- evaluationScheme(data_matrix, method = 'split', train = 0.8, given = 5)
```

### **Calculate a similarity matrix**

The similarity matrix allows you to understand how similar users are to each other. We are going to show the top 7. 

```{r similarity matrix}
# similarity matrix using the euclidean distance 
round(similarity(normalize(train[1:7]),method = 'euclidean'),3)

# similarity matrix using the cosine distance
round(similarity(normalize(train[1:7]),method = 'cosine'),3)

# similarity matrix using the pearson distance

similarity.matrix <- round(similarity(normalize(train[1:7]),method = 'pearson'),3)
```


### **User Based Collaborative Filter** 

Here, we try to search for lookalike tasters and offer wines based on what tasters with simiar taste profile has. This algorithm is very effective but takes a lot of time and resources. This type of filtering requires computing every customer pair information which takes time. So, for big base platforms, this algorithm is hard to put in place.


```{r UBCF}
recommenderRegistry$get_entry('UBCF', type = 'realRatingMatrix')

recom.ubcf <- Recommender(train,method='UBCF',parameter = list(method = 'Cosine'))
pred.ubcf <- predict(recom.ubcf,train,n = 1)

datatable(data.frame(mapply(c, 'Wine Recommendation' = getList(pred.ubcf), 
                     'Wine Rating' = getRatings(pred.ubcf),
                     'Avg Rating' = as.list(rowMeans(train)))))

recom.ubcf.2 <- Recommender( data = getData(es, 'train'),
                             method = 'UBCF')

pred.ubcf.2 <- predict(recom.ubcf.2,newdata = getData(es, 'known'), type = 'ratings')

calcPredictionAccuracy(pred.ubcf.2, data = getData(es, 'unknown'))
```

### **Popular Methods** 

The Popular method is one of the oldest approaches to recommendations involves recommmending the most popular item. Although less widely used today, we still see this in the form of Top 50 Music Hits, Billboard Top Hits, and New York Times Bestsellers. The underlying assumption is that if most people like a wine, a book or a song, you will like it too.

```{r Popular Method}
recommenderRegistry$get_entry("POPULAR", type ="realRatingMatrix")

# recommendation for normal data 
recom.popular <- Recommender(train,method='POPULAR')
pred.popular <- predict(recom.popular,data_matrix, n = 1)

# top recommendation 
datatable(
data.frame(mapply(c, 'Wine Recommendation' = getList(pred.popular), 
                  'Wine Rating' = getRatings(pred.popular),
                     'Avg Rating' = as.list(rowMeans(train)))))

recom.popular.2 <- Recommender( data = getData(es, 'train'),
                             method = 'POPULAR')

pred.popular.2 <- predict(recom.popular.2,newdata = getData(es, 'known'), type = 'ratings')

calcPredictionAccuracy(pred.popular.2, data = getData(es, 'unknown'))
```
 
### **Item Collaborative filtering** 

It is very similar to the previous algorithm, but instead of finding a tasters look alike, we try finding wine look alike. Once we have wine look alike matrix, we can easily recommend alike wine to a customer who has reviewed any wine the store. This algorithm requires far fewer resources than user-user collaborative filtering. Hence, for new tasters or customer, the algorithm takes far lesser time than user collaborate as we don’t need all similarity scores between customers. Amazon uses this approach in its recommendation engine to show related products which boost sales.


```{r}

recommenderRegistry$get_entry('IBCF', type = 'realRatingMatrix')

recom.ibcf <- Recommender(train,method='IBCF')
pred.ibcf <- predict(recom.ubcf,train, n = 1, type = 'topNList')

datatable(
data.frame(mapply(c, 'Receommendatoin' = getList(pred.ibcf), 
                     'Wine Rating' = getRatings(pred.ibcf),
                     'Avg Rating' = as.list(rowMeans(train))))
)

recom.ibcf.2 <- Recommender( data = getData(es, 'train'),
                             method = 'IBCF')

pred.ibcf.2 <- predict(recom.ibcf.2,newdata = getData(es, 'known'), type = 'ratings')

calcPredictionAccuracy(pred.ibcf.2, data = getData(es, 'unknown'))
```


The model with the lowest RMSE is Popular based method. Below we examine the problems with recommendation systems

### Challenges

**Data Sparsity**: When new items are added to system, they need to be rated by substantial number of users before they could be recommended to users who have similar tastes with the ones rated them.

**Cold Start Problem**: The “cold start" problem happens in recommendation systems due to the lack of information, on users or items. The Cold-Start problem is a wellknown issue in recommendation systems: there is relatively little information about each user, which results in an inability to draw inferences to recommend items to users. 

Lastly, the accuracy improves as the number of items per user. 

#4. Factors that influence Price

For this analysis, we are going to analyze the factor that impact prices from business owners perspectives. This analysis can be used to determine the types of wine, winery to build, where to dedicate resources to build a winery, and given that the business owner wants to target a specific market segment. 


### Test the correlation between the variables

To understand the factors that impact price. Prior to building the regression model, we wanted to understand the correlation among the variables, and we discovered that **`r winery.name`** & **`r title.name`**, and **`r region_1.name`** and **`r region_2.name`** are highly correlated. We removed `r region_2.name` and `r winery.name`, the assumption is that their impact would be captured by `r winery.name` and `r region_1.name`. 


```{r}
data.numeric <- dat

data.numeric$country <- as.numeric(as.factor(data.numeric$country))
data.numeric$designation <- as.numeric(as.factor(data.numeric$designation))
data.numeric$province <- as.numeric(as.factor(data.numeric$province))
data.numeric$region_1 <- as.numeric(as.factor(data.numeric$region_1))
data.numeric$region_2 <- as.numeric(as.factor(data.numeric$region_2))
data.numeric$taster_name <- as.numeric(as.factor(data.numeric$taster_name))
data.numeric$title <- as.numeric(as.factor(data.numeric$title))
data.numeric$variety <- as.numeric(as.factor(data.numeric$variety))
data.numeric$winery <- as.numeric(as.factor(data.numeric$winery))

correlation.value <- cor(data.numeric)
corrplot(correlation.value, method = 'square',type = 'lower',diag = F)
```

We built a linear regression model that analyzes the factors that impact price using `r country.name`, `r designation.name`, `r points.name`, `r province.name`, `r region_1.name`, `r taster.name`, `r variety.name`, and `r winery.name`. The analysis that the variables that impact price are `r designation.name`, `r province.name`, `r points.name`, `r region_1.name`, and `r variety.name`. It’s also important to note that the reviewers rating isn’t significant. Given that `r price.name` and `r points.name` have a moderate correlation, it would indicate that the reviewers are unbiased in their review. However, interpreting the variables presents a number of challenges: 

There are 7 categorical values in the dataset, and it becomes difficult to interpret the beta variables. The model is indicating that an increase in the `r designation.name` decreases the estimate. There are a number of ways to address including convert the character variables into factors, however, `r designation.name` has `r unique.designation` levels. 

The linear regression model is predicting a continuously variable output, for this audience, a small difference in price isn’t meaningful to analyze, i.e. the difference between a wine priced at 10.50 vs 11.00.  We want to be able to capture a larger differences in price. 

```{r}
# drop correlated values 
data.numeric2 <- data.numeric
data.numeric2$title <- NULL
data.numeric2$region_2 <- NULL

# Fit linear Model 
lm.model <- lm(price ~., data = data.numeric2)
lm.model.summary <- linear.regression.summary(lm.model)[,c(1,2,5:7)]
lm.model.summary[,2:5] <- round(lm.model.summary[,2:5],3)
datatable(lm.model.summary)
```

For the next phase of the analysis, we want to capture significant differences in the model and interpret the categorical variables. According to Wine Folly, there are 8 categories of wine: 

| Segment Name    | Price ($USD)  |Description                                                          |
| :-            | :-            | :----                                                                 |
| Extreme Value   | Less than 4   | Bulk wine                                                           |
| Value           | 4 - 10        | Basic quality bulk wines from large regions and producers           |
| Popular Premium | 10 - 15       | Large production, decent varietal wines, and blends                 |
| Premium         | 14 - 20       | Good, solid quality wines                                           |
| Super Premium   | 20 - 30       | Great, handmade wines from medium-large production wineries         |
| Ultra Premium   | 30 - 50       | Great quality, handmade, excellent-tasting wines                    |
| Luxury          | 50 - 100      | Excellent wines from wine regions made by near-top producers        |
| Super Luxury    | 100 - 200     | Wines from top producers from microsites                            |
| Icon            | 200+          | The pinnacle of wines, wineries, and microsites                     |


```{r }
dat$price <- cut2(x = dat$price,cuts = price.category.range, price.category.name)
levels(dat$price) <- price.category.name
price.count <- dat[,.(count = .N), by = price.name]

ggplot(data = price.count, aes(x = price, y = count)) + 
  geom_bar(stat = 'identity', fill = 'lightblue') + theme_classic() + 
  ggtitle('Price Category Distribution') + theme(legend.position = "bottom",
              text = element_text(size=15),
              axis.text.x = element_text(angle=0),
              plot.title = element_text(hjust = 0.5)) + xlab('Pricing Segment') + ylab('Count')
```

A deeper dive into the categorical variables reveals that a significant portion of the categorical variables have very few observations. Lack of sufficient variables greatly impacts the strength of analysis and conclusion. In the next section, we are going to filter variables with insufficient variables. 

How many unique variables are in `r designation.name`, `r province.name`, and `r variety.name`? 

```{r}
paste0('There are ', length(unique(dat$designation)), ' unique observation values in the dataset')
paste0('There are ', length(unique(dat$province)), ' unique province values in the dataset')
paste0('There are ', length(unique(dat$variety)), ' unique variety values in the dataset')
```

Given that we now have 8 dependent variables, a linear or logistic regression isn’t the best model to determine the factors that influence. After researching this problem, the best models that can address this situation are Multinomial regression and ordinal logit model.

**Ordinal Logistic Regression**

Ordinal logistic regression or (ordinal regression) is used to predict an ordinal dependent variable given one or more independent variables. Ordinal regression will enable us to determine which of our independent variables (if any) have a statistically significant effect on our dependent variable. For categorical independent variables, we can interpret the odds that one “group” have a higher or lower score on our dependent variable. For continuous independent variables, we are able to interpret how a single unit increase or decrease in that variable, is associated with the odds of our dependent variable having a higher or lower value We can also determine how well our ordinal regression model predicts the dependent variable.

Ordinal regression can only be performed under these 4 conditions: 

- The dependent variable is measured on an ordinal level: Yes, the dependent variables range from value to Icon.  Value is considered as a cheaper option and Icon is the most expensive 
- One or more of the independent variables are either continuous, categorical or ordinal: The idenpendent variables are categorical and continuous

- No Multi-collinearity - i.e. when two or more independent variables are highly correlated with each other: Yes, We have removed all the variables that are highly correlated with each other. 
 - Proportional Odds - i.e. that each independent variable has an identical effect at each cumulative split of the ordinal dependent variable. Yes, the price is measured in the same currency and has the dependent on the quality of the wine 

**Multinomial logistic regression**

Multinomial logistic regression is used to model nominal outcome variables, in which the log odds of the outcomes are modeled as a linear combination of the predictor variables. Below we use the multinom function from the nnet package to estimate a multinomial logistic regression model. There are other functions in other R packages capable of multinomial regression. We chose the multinom function because it does not require the data to be reshaped (as the mlogit package does) and to mirror the example code found in Hilbe’s Logistic Regression Models. The multinom package does not include p-value calculation for the regression coefficients, so we calculate p-values using Wald tests (here z-tests).

The ratio of the probability of choosing one outcome category over the probability of choosing the baseline category is often referred as relative risk (and it is sometimes referred to as odds, described in the regression parameters above). The relative risk is the right-hand side linear equation exponentiated, leading to the fact that the exponentiated regression coefficients are relative risk ratios for a unit change in the predictor variable. We can exponentiate the coefficients from our model to see these risk ratios.

*Note:* we did not calculate the associated p value for the MLR. the formula to calculate it is: `z <- summary(model)$coefficients/summary(model)$standard.errors` followed by `p <- (1 - pnorm(abs(z), 0, 1)) * 2`.This calculation is computationally expensive, and we ran it several time without any success. We will assume that the variables have a significant p value. 

#### Analyzing Province Impact

```{r echo = TRUE, results = 'hide'}
dat.notable.provice.count <- dat[, .N, by = province.name][N > province.min]
dat.notable.provice <- dat[get(province.name) %in% dat.notable.provice.count$province]
multinom.model.province <- nnet::multinom(formula = price ~ province, data = dat.notable.provice)
```

The model summary output has a the odds ratio of each province and their respective price segment. It can be interpreted as the odd ratio of finding wine in price segment in a province compared to the baseline. For example: Alasce provice has a 7x higher odds ratio of finding a wine in the value premium and Alasce vs the base Price Segmnent and province.   

```{r multinom model province}
multinom.model.province.coef <- round(data.frame(exp(coef(multinom.model.province)),2))
datatable(multinom.model.province.coef)
```

```{r}
# olr model for provice 
olr.model.province <- polr(price ~ points + province, data = dat.notable.provice, Hess = TRUE)

# store table
ctable.province <- coef(summary(olr.model.province))

# calculate and store p values
p.province <- round(pnorm(abs(ctable.province[, "t value"]), lower.tail = FALSE) * 2,5)

# combined table
ctable.province <- cbind(ctable.province, "p value" = p.province)

odd.ratio.province <- round(exp(ctable.province[,'Value']),3)
ctable.province <- cbind(ctable.province, 'Odds Ratio' = odd.ratio.province)

datatable(round(ctable.province,3))
```

Here we can examine the odd.ratio and the impact of the province on the dependent variables. The province that have the highest odd.ratio of producing a higher price are Burgundy, Champagne and Piedmont each with a odd ratio 21.362, 57.872 and 17.861, respectively. 

Also note that increase in points also increase the probability of being able to charge a higher price by 50% 

Using the combination of these models, provides a more comprehensive review of the impact that province has on price. 

The same models are repeated for `r designation.name` and `r variety.name`, and can be interepreted the same way. 

### Analyzing Designation impact 

```{r echo = TRUE, results = 'hide'}
dat.designation.count <- dat[,.N, by = designation.name][N >designation.min]
dat.designation <- dat[get(designation.name) %in% dat.designation.count$designation]
multinom.model.designation <- nnet::multinom(formula = price ~ designation, data = dat.designation)
```

```{r multinom model designation}
multinom.model.designation.coef <- round(data.frame(exp(coef(multinom.model.designation)),2))
datatable(multinom.model.designation.coef)
```

```{r}
# olr model for provice 
olr.model.designation <- polr(price ~ designation, data = dat.designation, Hess = TRUE)

# store table
ctable.designation <- coef(summary(olr.model.designation))

# calculate and store p values
p.designation <- round(pnorm(abs(ctable.designation[, "t value"]), lower.tail = FALSE) * 2,5)

# combined table
ctable.designation <- cbind(ctable.designation, "p value" = p.designation)
odd.ratio.designation <- round(exp(ctable.designation[,'Value']),3)
ctable.designation <- cbind(ctable.designation, 'Odds Ratio' = odd.ratio.designation)
ctable.designation <-data.frame(ctable.designation)

datatable(round(ctable.designation,3))
```

### Analyzing variety impact 

```{r echo = TRUE, results = 'hide'}
# variety  
dat.variety.count <- dat[,.N, by = variety.name][N >variety.min]
dat.variety <- dat[get(variety.name) %in% dat.variety.count$variety]
multinom.model.variety <- nnet::multinom(formula = price ~ variety, data = dat.variety)
```

```{r multinom model variety}
multinom.model.variety.coef <- round(data.frame(coef(multinom.model.variety)),2)
datatable(multinom.model.variety.coef)
```

```{r}
# olr model for provice 
olr.model.variety <- polr(price ~ variety, data = dat.variety, Hess = TRUE)

# store table
ctable.variety <- coef(summary(olr.model.variety))

# calculate and store p values
p.variety <- round(pnorm(abs(ctable.variety[, "t value"]), lower.tail = FALSE) * 2,5)

# combined table
ctable.variety <- cbind(ctable.variety, "p value" = p.variety)
odd.ratio.variety <- round(exp(ctable.variety[,'Value']),3)
ctable.variety <- cbind(ctable.variety, 'Odds Ratio' = odd.ratio.variety)

datatable(round(data.frame(ctable.variety),3))
```
 
We Analyzed the factors that impact price. In the introduction, we stated that the reader can use this analysis to determine where to place winery and what type of wine to select given that they are targetting a specific price range. The analysis provides the location with the highest or lowest odds based on the availability. The next question that the user needs to determine is should they build where there are numerous competitors or an area with fewer competition. 


# References
[1] Bernard Chen, Valentin V., James P., and Travis A. Wineinformatics: A Quantitative Analysis of Wine Reviewers. <i> Fermentation </i>. 2018 <br>

[2] Quandt, R.E. A note on a test for the sum of ranksums. <i>J. Wine Econ</i>. 2007, 2, 98-102. <br>

[3] tf-idf Model for Page Ranking. Retrieved from https://www.geeksforgeeks.org/tf-idf-model-for-page-ranking/. <br>

[4] Napa Valley Vintners. (n.d.). How to Read a Wine Label Infographic. Retrieved May 05, 2019, from https://napavintners.com/wines/how_to_read_a_wine_label.asp

[5] Aldrich, J. H., & Nelson, F. D. (1984). Linear probability, logit, and probit models. Thousand
Oaks, CA: Sage. 

[6] Croissant, Y. (2011). Package ‘mlogit’. http://cran.r-project.org/web/packages/mlogit/index.html 
Fox, J. (1984). Linear statistical models and related methods: With applications to social
research. New York: Wiley.

[7] Garson, G. D. (2011). “Logistic Regression”, from Statnotes: Topics in Multivariate Analysis.
http://faculty.chass.ncsu.edu/garson/pa765/statnote.htm.

[8] Hair, J. F., Anderson, R. E., Tatham, R. L., & Black, W. C. (1998). Multivariate Data Analysis
(5th ed.). Upper Saddle River, NJ: Prentice Hall. 

[9] Mertler, C. & Vannatta, R. (2002). Advanced and multivariate statistical methods (2nd ed.). Los
Angeles, CA: Pyrczak Publishing. 

[10] Schwab, J. A. (2002). Multinomial logistic regression: Basic relationships and complete
problems. http://www.utexas.edu/courses/schwab/sw388r7/SolvingProblems/

[11] Tabachnick, B. G. & Fidell, L. S. (2001). Using multivariate statistics (4th ed.). Needleham
Heights, MA: Allyn and Bacon. 

[12] Madhukar, M. (2014). Challenges & Limitation in Recommender Systems. International Journal of Latest Trends in Engineering and Technology (IJLTET), 4(3), 138-142. Retrieved May 01, 2019.

[13] HOME. (n.d.). Retrieved May 05, 2019, from https://stats.idre.ucla.edu/r/dae/multinomial-logistic-regression/

[14] HOME. (n.d.). Retrieved from https://stats.idre.ucla.edu/r/dae/ordinal-logistic-regression/
